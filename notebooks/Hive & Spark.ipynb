{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective (recap)\n",
    "- How can we parallelize this task?\n",
    "- How can we make this component more modular and reusable?\n",
    "- Can we profile this application and employ optimizations to improve the training or classification time or response time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Tips and Bestpractices\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Random sampling uses the RAND() function and LIMIT keyword to get the sampling of data as shown in the following example. The DISTRIBUTE and SORT keywords are used here to make sure the data is also randomly distributed among mappers and reducers efficiently. The ORDER BY RAND() statement can also achieve the same purpose, but the performance is not good:\n",
    "\n",
    "SELECT * FROM <Table_Name> DISTRIBUTE BY RAND() SORT BY RAND()\n",
    "LIMIT <N rows to sample>;\n",
    "\n",
    "\n",
    "Block sampling allows Hive to randomly pick up N rows of data, percentage (n percentage) of data size, or N byte size of data. The sampling granularity is the HDFS block size. Its syntax and examples are as follows:\n",
    "\n",
    "--Syntax\n",
    "SELECT * \n",
    "FROM <Table_Name> TABLESAMPLE(N PERCENT|ByteLengthLiteral|N ROWS) s;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Analysis\n",
    "\n",
    "- Hive provides the EXPLAIN and ANALYZE statements that can be used as utilities to check and identify the performance of queries.\n",
    "- Hive statistics are a collection of data that describe more details, such as the number of rows, number of files, and raw data size, on the objects in the Hive database. Statistics is a metadata of Hive data. Hive supports statistics at the table, partition, and column level. These statistics serve as an input to the Hive Cost-Based Optimizer (CBO), which is an optimizer to pick the query plan with the lowest cost\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions\n",
    "Hive partitioning is one of the most effective methods to improve the query performance on larger tables. The query with partition filtering will only load the data in the specified partitions (subdirectories), so it can execute much faster than a normal query that filters by a non-partitioning field. ”\n",
    "\n",
    "**Partitions by date and time: **\n",
    "Use date and time, such as year, month, and day (even hours), as partition keys when data is associated with the time dimension\n",
    "\n",
    "**Partitions by locations: **\n",
    "Use country, territory, “state, and city as partition keys when data is location related\n",
    "\n",
    "**Partitions by business logics:**\n",
    "Use department, sales region, applications, customers, and so on as partitioned keys when data can be separated evenly by some business logic”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel execution\n",
    "\n",
    "By default, Hadoop launches a new JVM for each map or reduce job and runs the map or reduce task in parallel. When the map or reduce job is a lightweight job running only for a few seconds, the JVM startup process could be a significant overhead.\n",
    "\n",
    "To reuse JVM by sharing the JVM to run mapper/reducer serially instead of parallel. JVM reuse applies to map or reduce tasks in the same job. Tasks from different jobs will always run in a separate JVM. To enable the reuse, we can set the maximum number of tasks for a single job for JVM reuse using the mapred.job.reuse.jvm.num.tasks property. Its default value is 1:\n",
    "\n",
    "jdbc:hive2://> SET mapred.job.reuse.jvm.num.tasks=5;\n",
    "We can also set the value to –1 to indicate that all the tasks for a job will run in the same JVM.\n",
    "\n",
    "\n",
    "Hive queries commonly are translated into a number of stages that are executed by the default sequence. These stages are not always dependent on each other. Instead, they can run in parallel to save the overall job running time. We can enable this feature with the following settings:\n",
    "\n",
    "jdbc:hive2://> SET hive.exec.parallel=true; -- default false\n",
    "jdbc:hive2://> SET hive.exec.parallel.thread.number=16; \n",
    "-- default 8, \n",
    "it defines the max number for running in parallel\n",
    "Parallel execution will increase the cluster utilization. If the utilization of a cluster is already very high, parallel execution will not help much in terms of overall performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions\n",
    "\n",
    "Hive defines the following three types of UDF:\n",
    "\n",
    "**UDFs:** \n",
    "These are regular user-defined functions that operate row-wise and output one result for one row, such as most built-in mathematic and string functions.\n",
    "\n",
    "**UDAFs:** \n",
    "These are user-defined aggregating functions that operate row-wise or group-wise and output one row or one row for each group as a result, such as the MAX and COUNT built-in functions.\n",
    "\n",
    "**UDTFs:**\n",
    "These are user-defined table-generating functions that also operate row-wise, but they produce multiple rows/tables as a result, such as the EXPLODE function. UDTF can be used either after SELECT or after the LATERAL VIEW statement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See assignments at the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Tips and Bestpractices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Tips\n",
    "- Don’t collect large RDDs\n",
    "- Don't use count() when you don't need to return the exact number of rows\n",
    "- Use coalesce to repartition in decrease number of partition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Picking the Right Operators\n",
    "\n",
    "- Avoid List of Iterators \n",
    "    newRDD = textRDD.map(line => line.split(\",\"))\n",
    "\n",
    "The issue here is the returned RDD will be an iterator composed of iterators. What we want is the individual values obtained after calling the split function. \n",
    "\n",
    "instead use flatmap\n",
    "\n",
    "\n",
    "al inputData = sc.parallelize (Array (\"foo,bar,baz\", \"larry,moe,curly\", \"one,two,three\") ).cache ()\n",
    "\n",
    "val mapped = inputData.map (line => line.split (\",\") )\n",
    "val flatMapped = inputData.flatMap (line => line.split (\",\") )\n",
    "\n",
    "val mappedResults = mapped.collect ()\n",
    "val flatMappedResults = flatMapped.collect ();\n",
    "\n",
    "println (\"Mapped results of split\")\n",
    "println (mappedResults.mkString (\" : \") )\n",
    "\n",
    "println (\"FlatMapped results of split\")\n",
    "println (flatMappedResults.mkString (\" : \") )\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "Mapped results of split\n",
    "[Ljava.lang.String;@45e22def : [Ljava.lang.String;@6ae3fb94 : [Ljava.lang.String;@4417af13\n",
    "FlatMapped results of split\n",
    "foo : bar : baz : larry : moe : curly : one : two : three\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "In other words, we need an Array[String]not an Array[Array[String]].\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Avoid groupByKey when performing a group of multiple items by key\n",
    "\n",
    "In the case of a groupByKey call, every single key-value pair will be shuffled accross the network with identical keys landing on the same reducer. To state the obvious, when grouping by key, the need for all matching keys to end up on the same reducer can’t be avoided. But one optimization we can attempt is to combine/merge values so we end up sending fewer key-value pairs in total. \n",
    "\n",
    "Shuffling can be a great bottleneck. Having many big HashSet's (according to your dataset) could also be a problem. However, it's more likely that you'll have a large amount of ram than network latency which results in faster reads/writes across distributed machines.\n",
    "\n",
    "Here are more functions to prefer overgroupByKey:\n",
    "\n",
    "combineByKey\n",
    "can be used when you are combining elements but your return type differs from your input value type. \n",
    "\n",
    "When using combineByKey values are merged into one value at each partition then each partition value is merged into a single value. It’s worth noting that the type of the combined value does not have to match the type of the original value and often times it won’t be.\n",
    "\n",
    "\n",
    "foldByKey\n",
    "merges the values for each key using an associative function and a neutral \"zero value\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Avoid groupByKey when performing an associative reductiove operation\n",
    "\n",
    "For example, rdd.groupByKey().mapValues(_.sum) will produce the same results as rdd.reduceByKey(_ + _). \n",
    "However, the former will transfer the entire dataset across the network, while the latter will compute local sums for each key in each partition and combine those local sums into larger sums after shuffling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use Broadcast variable\n",
    "- Joining a large and a small RDD\n",
    "- Joining a large and a medium size RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which storage level to choose\n",
    "- Avoiding Shuffle \"Less stage, run faster\"\n",
    "- Use the right level of parallelism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialization\n",
    "\n",
    "Serialization plays an important role in the performance of any distributed application. Formats that are slow to serialize objects into, or consume a large number of bytes, will greatly slow down the computation. Often, this will be the first thing you should tune to optimize a Spark application. The Java default serializer has very mediocre performance regarding runtime as well as the size of its results. Therefore the Spark team recommends to use the Kryo serializer instead."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val conf = new SparkConf().setAppName(...).setMaster(...)\n",
    "      .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      .set(\"spark.kryoserializer.buffer.max\", \"128m\")\n",
    "      .set(\"spark.kryoserializer.buffer\", \"64m\")\n",
    "      .registerKryoClasses(\n",
    "        Array(classOf[ArrayBuffer[String]], classOf[ListBuffer[String]])\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments (Hive)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order to not get lost in the details, here is what we want to achieve from a high-level perspective.\n",
    "\n",
    "Set up small example Hive table within some database.\n",
    "Create a virtual environment and upload it to Hive’s distributed cache.\n",
    "Write the actual UDAF as Python script and a little helper shell script.\n",
    "Write a HiveQL query that feeds our example table into the Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and uploading a virtual environment -- Hive"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order to prepare a proper virtual environment we need to execute the following steps on an OS that is binary compatible to the OS on the Hive cluster. Typically any recent 64bit Linux distribution will do.\n",
    "\n",
    "We start by creating an empty virtual environment with:\n",
    "\n",
    "virtualenv —no-site-packages -p /usr/bin/python3 venv\n",
    "\n",
    "assuming that virtualenv was already installed with the help of pip. Note that we explicitly ask for Python 3. Who uses Python 2 these days anyhow?\n",
    "\n",
    "The problem with the activate script of a virtual environment is that its path is hard-coded. We change that by replacing the line\n",
    "\n",
    "VIRTUAL_ENV=\"your/path/to/venv\"\n",
    "with\n",
    "\n",
    "HERE=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\n",
    "VIRTUAL_ENV=\"$( readlink -f \"${HERE}/../\" )\"\n",
    "in ./venv/bin/activate. Additionally, we replace in pip the shebang line, i.e. replacing\n",
    "\n",
    "#!/your/path/to/venv/venv/bin/python3\n",
    "with\n",
    "\n",
    "#!/usr/bin/env python\n",
    "This will help us later when we call pip list for debugging reasons.\n",
    "\n",
    "We activate the virtual environment and install Pandas in it.\n",
    "\n",
    "source venv/bin/activate\n",
    "\n",
    "pip install numpy pandas\n",
    "\n",
    "This should install Pandas and all its dependencies into our virtual environment. No we package the virtual environment for later deployment in the distributed cache:\n",
    "\n",
    "cd venv\n",
    "\n",
    "tar cvfhz ../venv.tgz ./\n",
    "\n",
    "cd ..\n",
    "\n",
    "Be aware that the archive was created with the actual content at its root so when unpacking there will be no directory holding the actual content. We also used the parameter h to package linked files.\n",
    "\n",
    "Now we push the archive to HDFS so that later Hive’s data nodes will be able to find it:\n",
    "\n",
    "hdfs dfs -put venv.tgz /tmp\n",
    "\n",
    "The directory /tmp should be changed accordingly. One should also note that in principle the same procedure should also be possible with conda environments. In practice though, it might be a bit more involved since the activation of a conda environment (what we need to do later) assumes an installation of at least miniconda which might not be available on the data nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEP = '\\t'\n",
    "NULL = '\\\\N'\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def read_input(input_data):\n",
    "    for line in input_data:\n",
    "        yield line.strip().split(SEP)\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, stream=sys.stderr)\n",
    "    data = read_input(sys.stdin)\n",
    "    ## TODO: Your function invocation goes here\n",
    "    ## print the output don't return it\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a bash script to load the virtual environment and the python script"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "set -e\n",
    "(>&2 echo \"Begin of script\")\n",
    "source ./venv.tgz/bin/activate\n",
    "(>&2 echo \"Activated venv\")\n",
    "(>&2 pip list --format=columns --no-cache-dir)\n",
    "python udaf.py\n",
    "(>&2 echo \"End of script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move scripts and udf's to hdfs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hdfs dfs -put udaf.py /tmp\n",
    "\n",
    "hdfs dfs -put udaf.sh /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DELETE ARCHIVE hdfs:///tmp/venv.tgz;\n",
    "ADD ARCHIVE hdfs:///tmp/venv.tgz;\n",
    "DELETE FILE hdfs:///tmp/udaf.py;\n",
    "ADD FILE hdfs:///tmp/udaf.py;\n",
    "DELETE FILE hdfs:///tmp/udaf.sh;\n",
    "ADD FILE hdfs:///tmp/udaf.sh;\n",
    "\n",
    "USE tmp;\n",
    "SELECT TRANSFORM(arg1, arg2, arg3) USING 'udaf.sh' AS (arg1 STRING, arg2 FLOAT, arg3 FLOAT)\n",
    "  FROM (SELECT * FROM foo CLUSTER BY arg3) AS TEMP_TABLE;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assingment 1\n",
    "Take any data set and perform a data shuffle and split the data into two parts (90/10 split for training/testing).\n",
    "\n",
    "### Solution\n",
    "\n",
    "Find the total number of records and use it in conjunction with limit.\n",
    "\n",
    "SELECT * FROM DISTRIBUTE BY RAND() SORT BY RAND() LIMIT ;\n",
    "\n",
    "TIP: if you just need to do preliminary analysis do :\n",
    "SELECT * FROM TABLESAMPLE (10 PERCENT);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "\n",
    "Download data from http://stat-computing.org/dataexpo/2009/the-data.html for year 2007\n",
    "\n",
    "a. Write a Hive query wich imports the data and filters out all the delayed flights.\n",
    "b. Write a UDF to find out the hour of departure from the field CRSDepTime\n",
    "c. Write a UDF to find out the days from the nearest holiday for the date(Year, Month, DayOfMonth) fields.\n",
    "\n",
    "Note : treat the following dates as holidays\n",
    "    \n",
    "holidays = [\n",
    "        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n",
    "        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25)]\n",
    "\n",
    "### Solution\n",
    "\n",
    " CREATE EXTERNAL TABLE IF NOT EXISTS data(\n",
    "    > Year INT, Month INT, DayOfMonth INT, DayOfWeek INT, DepTime STRING, CRSDepTime STRING, \n",
    "    > ArrTime timestamp, CRSArrTime timestamp, Carrier STRING, FlightNum STRING, TailNum STRING, \n",
    "    > ActualElapsedTime timestamp, CRSElapsedTime timestamp, AirTime STRING, ArrDelay STRIN, DepDelay INT,\n",
    "    >  Origin STRING, Dest STRING, Distance INT, TaxiIn STRING, TaxiOut STRING, Cancelled INT, CancellationCode STRING, Diverted STRING, \n",
    "    > CarrierDelay STRING, WeatherDelay STRING, NASDelay STRING, SecurityDelay STRING, LateAircraftDelay STRING)\n",
    "    > COMMENT 'Flight Data'\n",
    "    > ROW FORMAT DELIMITED\n",
    "    > FIELDS TERMINATED BY ','\n",
    "    > STORED AS TEXTFILE\n",
    "    > LOCATION '/user/username/flightdata.csv';\n",
    "    \n",
    "### Note the difference wrt PIG in loading data all datatypes have to be provided.\n",
    "\n",
    "### You can store the file in other types like:\n",
    "\n",
    "- Text file : All data are stored as raw text using the Unicode standard.\n",
    "\n",
    "- Sequence file : The data are stored as binary key/value pairs.\n",
    "\n",
    "- RCFile : All data are stored in a column optimized format (instead of row optimized).\n",
    "\n",
    "- ORC : An optimized row columnar format that can significantly improve Hive performance.\n",
    "\n",
    "- Parquet : A columnar format that provides portability to other Hadoop tools including Hive, Drill, Impala, Crunch, and Pig.\n",
    "\n",
    "\n",
    "Create another table named mydata with same schema but change STORED AS TEXTFILE and remove location.\n",
    "\n",
    "INSERT OVERWRITE TABLE mydata SELECT * FROM data;\n",
    "\n",
    "This is an easy and fast way to convert your data in parallel to columnar storage."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assingment 3 Natural Language Processing, Text Processing\n",
    "\n",
    "\n",
    "Get data from https://www.kaggle.com/zynicide/wine-reviews/data\n",
    "From the wine data find the top 5 words used to describe each wine for each country.\n",
    "Note: You should write Python UDF/UADF for NLP proceessing.\n",
    "\n",
    "## Solution\n",
    "Function implementation remains the same just the process for Hive deployment is different as discussed above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assignment 4\n",
    "# big data - list all google books data links https://gist.github.com/Kroid/d92a242b374043353ea6\n",
    "\n",
    "Finds word frequencies (probability that a random word is the given word) using the Google Books corpus.\n",
    "\n",
    "Download data using : \n",
    "- curl http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-1gram-20120701-1.gz -o gb.gz\n",
    "- gunzip gb.gz\n",
    "\n",
    "\n",
    "a. Load the word occurrence data\n",
    "b. Write a UDF for : Ignore case, combining the occurrences of different capitalizations of the same word ex. \"quantity\", \"Quantity\", and \"QUANTITY\" all get combined    \n",
    "c. Write a UDF for : Finding the word frequencies (probability that a random word is the given word) by normalizing the occurrences column against the total number of occurrence\n",
    "\n",
    "### Solution\n",
    "\n",
    "For udf's follow the instructions at the very top. Create a seperate python file for each UDF and the UDF's are as follows:\n",
    "\n",
    "def main():\n",
    "    data = read_input(sys.stdin)\n",
    "    # grouping by words\n",
    "    for group in groupby(data, itemgetter(0)):\n",
    "        group = [word, np.nan if occurances == NULL else int(occurances))\n",
    "                 for word, occurances in group]\n",
    "        df = pd.DataFrame(group, columns=('word', 'occurances'))\n",
    "        output = [word, df['occurances'].sum()]\n",
    "        print(SEP.join(str(o) for o in output))\n",
    "\n",
    "SELECT TRANSFORM(word, occurances) USING 'udaf.sh' AS (word STRING, occurances INT)\n",
    "  FROM (SELECT * FROM data CLUSTER BY word) AS TEMP_TABLE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Assingment 1\n",
    "Take any data set and perform a data shuffle and split the data into two parts (90/10 split for training/testing).\n",
    "\n",
    "### Solution\n",
    "using randomSplit function\n",
    "weights = [.8, .2]\n",
    "seed = 42 \n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawTestData = data.randomSplit(weights, seed)\n",
    "\n",
    "or \n",
    "\n",
    "val Array(training, test) = data.randomSplit(Array(0.9, 0.1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assingment 2\n",
    "Download data from http://stat-computing.org/dataexpo/2009/the-data.html for year 2007\n",
    "\n",
    "a. Write a Hive query wich imports the data and filters out all the delayed flights.\n",
    "b. Write a UDF to find out the hour of departure from the field CRSDepTime\n",
    "c. Write a UDF to find out the days from the nearest holiday for the date(Year, Month, DayOfMonth) fields.\n",
    "\n",
    "Note : treat the following dates as holidays\n",
    "    \n",
    "holidays = [\n",
    "        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n",
    "        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25)]\n",
    "\n",
    "\n",
    "# Solution\n",
    "\n",
    "import org.apache.spark.sql\n",
    "\n",
    "import java.text.SimpleDateFormat\n",
    "import java.util.Calendar\n",
    "import java.util.Date\n",
    "import java.lang.Math\n",
    "\n",
    "\n",
    "val data = spark.read.csv(\"\"\"data-2007.csv\"\"\")\n",
    "\n",
    "val hour_udf = udf((s:String) => {\n",
    "    \"%04d\".format(s).substring(2)\n",
    "})\n",
    "\n",
    "val format = new java.text.SimpleDateFormat(\"yyyy/MM/dd\")\n",
    "val holidays = Array(format.parse(\"2007/01/01\"), format.parse(\"2007/01/15\"), format.parse(\"2007/02/19\"), format.parse(\"2007/05/28\"), format.parse(\"2007/06/07\"),format.parse(\"2007/07/04\"), format.parse(\"2007/09/03\"),format.parse(\"2007/10/08\"),format.parse(\"2007/11/11\"), format.parse(\"2007/11/22\"), format.parse(\"2007/12/25\"))\n",
    "\n",
    "val nearest_holiday_udf = udf((s:String) => {\n",
    "    val format = new java.text.SimpleDateFormat(\"yyyy/MM/dd\")\n",
    "    val t = format.parse(s)\n",
    "    holidays.map(h => java.lang.Math.abs((h.getTime() - t.getTime()) / (1000 * 60 * 60 * 24))).sortWith(_ < _).head\n",
    "})\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "# big data - list all google books data links https://gist.github.com/Kroid/d92a242b374043353ea6\n",
    "\n",
    "Finds word frequencies (probability that a random word is the given word) using the Google Books corpus.\n",
    "\n",
    "Download data using : \n",
    "- curl http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-1gram-20120701-1.gz -o gb.gz\n",
    "- gunzip gb.gz\n",
    "\n",
    "\n",
    "a. Load the word occurrence data\n",
    "b. Write a UDF for : Ignore case, combining the occurrences of different capitalizations of the same word ex. \"quantity\", \"Quantity\", and \"QUANTITY\" all get combined    \n",
    "c. Write a UDF for : Finding the word frequencies (probability that a random word is the given word) by normalizing the occurrences column against the total number of occurrence\n",
    "\n",
    "### Solution\n",
    "\n",
    "val data = spark.read.csv(\"\"\"worddata.csv\"\"\")\n",
    "\n",
    "\n",
    "val frequency = data.withColumn(\"transformed_word\", lower_udf($\"word\"))\n",
    "    .groupBy('transformed_word').count()\n",
    "\n",
    "val total_words = frequency.groupBy($\"transformed_word\").count().select(\"count\").groupBy().sum().collect()(0).getLong(0)\n",
    "val prob = frequency.withColumn('probability', $\"count\"/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
